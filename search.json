[
  {
    "objectID": "blog_start.html",
    "href": "blog_start.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nRegression standardization with the rstpm2 package\n\n\n\n\n\n\nR\n\n\nregression-standardization\n\n\nsurvival-analysis\n\n\n\n\n\n\n\n\n\nJul 2, 2024\n\n\nMichael Dismorr\n\n\n\n\n\n\n\n\n\n\n\n\nRegression standardization with the mexhaz package\n\n\n\n\n\n\nR\n\n\nregression-standardization\n\n\nsurvival-analysis\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\nMichael Dismorr\n\n\n\n\n\n\n\n\n\n\n\n\nImplement EuroSCORE II risk score in R\n\n\n\n\n\n\nR\n\n\nrisk-score\n\n\neuroscore\n\n\nlogistic-regression\n\n\nodds\n\n\nodds-ratios\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\nMichael Dismorr, Ruixin Lu\n\n\n\n\n\n\n\n\n\n\n\n\nPublication-ready tables with flextable and own theme in R\n\n\n\n\n\n\nR\n\n\npublication-ready\n\n\ntableone\n\n\nflextable\n\n\ntables\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nMichael Dismorr\n\n\n\n\n\n\n\n\n\n\n\n\nAge-, and sex-adjusted incidence rates\n\n\n\n\n\n\nR\n\n\nincidence-rate\n\n\npoisson-regression\n\n\nsurvival-analysis\n\n\n\n\n\n\n\n\n\nNov 15, 2021\n\n\nMichael Dismorr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/new_project/new_project_1.html",
    "href": "guides/new_project/new_project_1.html",
    "title": "New project workflow",
    "section": "",
    "text": "This guide will cover a suggested workflow for when you want to start working on a new project.\nSpecifically, it will cover:\n\nUsing the usethis package\nCreating a new R-project\nUsing it with git\nPushing it to a remote repositry using github\nUsing renv to keep track of the packages used in the project\nSuggesting a simple git workflow for collaborating on your project\n\nLet’s get started!",
    "crumbs": [
      "Guides",
      "New project",
      "New project workflow"
    ]
  },
  {
    "objectID": "guides/collaborate/collaborate_1.html",
    "href": "guides/collaborate/collaborate_1.html",
    "title": "Clone project using gh-cli",
    "section": "",
    "text": "This section will guide you through how to set up when you’ve been invited to collaborate on a new project. As I mentioned in the previous section, I like to keep a folder structure to keep track of the repo owner, that is, who created and likely leads the project. If I have invited you to collaborate, you should make sure you have a folder like this ~/my_git/Michael.\nFirst, open your terminal using spotlight by pressing Command+SpaceCommand+Space and type in terminal.\n\n\n\nOpen terminal using spotlight\n\n\nType ls to confirm the my_git folder exists. If it does type cd my_git to go into it, otherwise first type mkdir my_git to create it. Once in the my_git folder, type ls again to see if the Michael folder exists, otherwise create it using mkdir and go into it using cd.\n\n\n\nGoing into the my_git folder\n\n\nNow, we are going to clone to repository to our computer. If you already know the GitHub name and repo name that you have been invited to, you can type:\ngh repo clone Username/repo_name\nI often find myself not knowing either the username nor the repo name, so I list all repos I have access to using the repo-name alias we set up in the Setup Guide.\ngh repo-names\nOnce it has finished cloning, go into the repo_name folder and then open RStudio.\ncd repo-name\nopen repo-name.RProj",
    "crumbs": [
      "Guides",
      "Collaborate",
      "Clone project using gh-cli"
    ]
  },
  {
    "objectID": "guides/collaborate/collaborate_3.html",
    "href": "guides/collaborate/collaborate_3.html",
    "title": "Working with git",
    "section": "",
    "text": "When you have closed RStudio after your work, it’s time to push your changes to GitHub using git, so that your collaborators can see and use what you have been working on.\nIn the terminal, add any new files using git add ., the . here can be interpreted as “everything”. Next, it is time to give our changes a message, git commit -m \"Added new analyses\". Lastly, we push it to Github git push.\n\n\n\nAdding changes with git\n\n\nThis is a diagram depicting a typical workflow with git. Typically, you would start your day at 1 and finish at 6.\n\n\n\n\n\n\n\nCircularProcess\n\nGit workflow\n\n\nA\n\n1\n Pull latest changes\ngit pull\n\n\n\nB\n\n2\n Update any new dependencies\nrenv::restore()\n\n\n\nA-&gt;B\n\n\nopen project_name.RProj\n\n\n\nC\n\n3\n Register any new packages\nrenv::snapshot()\n\n\n\nB-&gt;C\n\n\nDo your work\n\n\n\nD\n\n4\n Add any untracked files\ngit add .\n\n\n\nC-&gt;D\n\n\nClose RStudio and go back to terminal\n\n\n\nE\n\n5\n Commit your changes\ngit commit -m \"Adds new analyses\"\n\n\n\nD-&gt;E\n\n\n\n\n\nF\n\n6\n Push your changes to GitHub\ngit push\n\n\n\nE-&gt;F\n\n\n\n\n\nF-&gt;A\n\n\nStart over",
    "crumbs": [
      "Guides",
      "Collaborate",
      "Working with git"
    ]
  },
  {
    "objectID": "guides/setup/setup_4.html",
    "href": "guides/setup/setup_4.html",
    "title": "Installing gh-cli",
    "section": "",
    "text": "Now, we will use brew to install the github command-line program, gh-cli. This command line program will allow you to login, sync, and clone the repositories you have access to on GitHub.\nType brew install gh in your terminal and press EnterEnter.\n\n\n\nInstalling gh with brew\n\n\nWhen the command finishes, type gh auth login in the terminal. This will log you in to your github account. Chose GitHub, HTTPS, Yes to authenticate git, and login via browser.\n\n\n\ngh login\n\n\nTo confirm that your login worked, type in gh auth status and press EnterEnter.\nNow we will create an alias, or our own defined command to use with gh. Typen in the following in the terminal:\ngh alias set repo-names '!gh api user/repos --paginate | jq \".[] | .full_name\"'\nIn order for this alias to work, we need to install the jq command using brew:\nbrew install jq\nWhen you run gh repo-names, it will show you a list of your GitHub repositories, as well as those you have been invited to collaborate on. This differs from the gh repo list command which will only show you your own repos. We will get back to this at a later guide.\nWe will also take some time to set up your git command if you have not used it before. Run the following commands (and replace Firstname and Lastname with your actual names, and example@email.com with your actual email that you use for GitHub):\n\n\nSet default editor\n\ngit config --global core.editor \"nano -w\"\n\n\n\nSet your name\n\ngit config --global user.name \"Firstname Lastname\"\n\n\n\nSet your email\n\ngit config --global user.email \"example@email.com\"",
    "crumbs": [
      "Guides",
      "Setup Guide",
      "Installing gh-cli"
    ]
  },
  {
    "objectID": "guides/setup/setup_6.html",
    "href": "guides/setup/setup_6.html",
    "title": "Installing RStudio",
    "section": "",
    "text": "Now, it’s finally time to install RStudio. Go to https://posit.co, and click on “Download Rstudio”. Don’t click the button that says “Download Rstudio Server”, but use the regular desktop version.\nSince you have already installed R, go ahead and click on “Download Rstudio Desktop for macOS 12+”\n\n\n\nInstall RStudio\n\n\nDrag the RStudio icon into your Applications folder. Once the installation finishes, eject the .dmg file and move it to the trash.\nThen open RStudio either by double clicking on the RStudio icon in the Applications folder, or using spotlight by pressing Command+SpaceCommand+Space and typing rstudio.\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t use spotlight often, the first time you use it, it might take some time to find the applications you’re looking for. This will be much faster the next time you use it, and definitely faster than clicking your way through finder to your Applications folder.\n\n\n\n\n\nOpening RStudio through spotlight\n\n\nOnce opened, click the Tools menu, and then click Global Options.\nIn the General section, uncheck Restore most recently opened project at startup,\nRestore previously open source documents at startup, Restore .RData inte workspace at  startup:, and select Save workspace to .RData on exit: Never.\nThen click apply.\n\n\n\nGlobal Options\n\n\nNow we will install our first two packages, renv and usethis. renv will keep track of the dependencies in our future projects, and usethis will assist us in our workflow when creating new projects (or R-packages). In the console type:\n\n\nInstall renv\n\ninstall.packages(\"renv\")\n\n\n\nInstall usethis\n\ninstall.packages(\"usethis\")\n\nThis marks the end of this Setup guide. In the following section, we will go through a suggested workflow when starting a new project.",
    "crumbs": [
      "Guides",
      "Setup Guide",
      "Installing RStudio"
    ]
  },
  {
    "objectID": "guides/setup/setup_3.html",
    "href": "guides/setup/setup_3.html",
    "title": "Installing Homebrew",
    "section": "",
    "text": "Now it’s time to install Homebrew. Homebrew is a packet manager, and you can use it as a command in your terminal to install other programs needed for development, including other commands.\nFirst, go to the Homebrew webpage on https://brew.sh. Then, click on the clipboard icon to the left where I have marked the image “Click here”\n\n\n\nHomebrew webpage\n\n\nThen, open you terminal. This program can be found in your Applications folder, but it is often faster to use Spotlight by pressing Command+SpaceCommand+Space and then typing terminal. Then select the Terminal application and press EnterEnter.\n\n\n\nSpotlight\n\n\nNow, paste in the command you copied from the Homebrew webpage by pressing Command+VCommand+V and then press EnterEnter.\nYou might be promted to enter your computer password, do this and then press EnterEnter.\n\n\n\n\n\n\nBeware\n\n\n\nYou will not see that you are entering anything in the terminal, that’s the default look when entering sensitive information such as passwords. Don’t try and enter it again, as it will be entered two times.\n\n\n\n\n\nTerminal\n\n\nIf no errors were encountered, you have now successfully installed the brew command. While we are in the terminal, let’s prepare a folder structure we will use later on. First, we will create the my_git folder where all yours and others projects will be placed.\n\n\nCreate my_git folder\n\nmkdir my_git\n\nThen, we will move into that folder, and create a new folder with your first name. This is where we will create your first project later.\n\n\nCreate your folder within the my_git folder\n\ncd my_git\nmkdir Michael",
    "crumbs": [
      "Guides",
      "Setup Guide",
      "Installing Homebrew"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michael Dismorr",
    "section": "",
    "text": "I am a M.D., PhD and postdoctoral researcher at Karolinska Institutet in the cardiothoracic surgery research group.\nThis page has been migrated from using blogdown and Netlify, to Quarto and github pages.\nHere I will primarily post guides and tutorials related to epidemiology in R. Topics will range from survival analysis with regression standardization and inverse probability of treatment weighting (IPTW) to more workflow related guides. Feel free to reach out with comments or suggestions.\nThe blog section covers short tutorial-style posts, mostly regarding biostatistics and survival analysis in R.\nThe guides section is aimed towards new members of our research group (and anyone else who finds the information helpful), and is more “course styled”.\nFor more content like this, check out R-bloggers."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Michael Dismorr",
    "section": "",
    "text": "I am a M.D., PhD and postdoctoral researcher at Karolinska Institutet in the cardiothoracic surgery research group.\nThis page has been migrated from using blogdown and Netlify, to Quarto and github pages.\nHere I will primarily post guides and tutorials related to epidemiology in R. Topics will range from survival analysis with regression standardization and inverse probability of treatment weighting (IPTW) to more workflow related guides. Feel free to reach out with comments or suggestions.\nThe blog section covers short tutorial-style posts, mostly regarding biostatistics and survival analysis in R.\nThe guides section is aimed towards new members of our research group (and anyone else who finds the information helpful), and is more “course styled”.\nFor more content like this, check out R-bloggers."
  },
  {
    "objectID": "index.html#intrests",
    "href": "index.html#intrests",
    "title": "Michael Dismorr",
    "section": "Intrests",
    "text": "Intrests\n\nR programming\nBiostatistics\nSurvival analysis\nGlobal health\nMedTech\nAI"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Michael Dismorr",
    "section": "Experience",
    "text": "Experience\nPostdoctoral researcher | 2022 - Current\nKarolinska Institutet | Stockholm, Sweden\nCo-founder | 2022 - Current\nDismorr Medical Ltd. | Nairobi, Kenya\nPhD student | 2017 - 2022\nKarolinska Institutet | Stockholm, Sweden\nSurgeon, licensed physician | 2018 - 2020\nKarolinska University Hospital, Dept. Cardiothoracic surgery | Stockholm, Sweden\nMedical internship | 2017 - 2018\nKarolinska University Hospital | Stockholm, Sweden\nJunior surgeon | 2016 - 2017\nKarolinska University Hospital, Dept. Cardiothoracic surgery | Stockholm, Sweden"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Michael Dismorr",
    "section": "Education",
    "text": "Education\nPhD, Cardiothoracic surgery epidemiology | 2017 - 2022\nKarolinska Institutet | Stockholm, Sweden\nMedical school, MD | 2011 - 2016\nKarolinska Institutet | Stockholm, Sweden\nGlobal health, field studies | 2015 - 2016\nKarolinska Institutet | Tanzania"
  },
  {
    "objectID": "posts/publication-ready-tables-with-flextable-and-own-theme/index.html",
    "href": "posts/publication-ready-tables-with-flextable-and-own-theme/index.html",
    "title": "Publication-ready tables with flextable and own theme in R",
    "section": "",
    "text": "In this post, I will show you how to make publication-ready using a combination of the flextable package and a function I’ve written to customize them according to my default layout. This will minimize the need to edit in MS Word, and copy and paste from Excel to Word when exporting tables from R. In this example, I will use the tableone package to show how my customtab() function can be used to export Table 1.\nThe flextable package is a great package to generate word tables from R output. Together with the officer package from the same creator, it allows you to make most of the customization needed immediately in R, and thus let’s you skip this potentially annoying step in MS Word. The tableone package is a great package for, you guessed it, generating Table 1s. It allows for p-value calculations, SMDs, and generally just the output that you need.\nFor demonstration, I will use the gbsg dataset from the from the survival package. If you just want to try out the customtab() function, it can be downloaded here.\nOK, let’s begin!\n\nlibrary(survival) # only needed for the dataset in this example\nlibrary(dplyr) # to modify the needed dataframe\nlibrary(tibble) # for rownames_to_column() function\nlibrary(stringr) # for str_squish()\nlibrary(flextable)\nlibrary(officer)\nlibrary(forcats)\nlibrary(tableone)\nsource(\"customtab.R\")\n\nLoad the dataset:\n\nbreast &lt;- survival::gbsg\n\nRunning ?survival::gbsg gives:\n\nThe gbsg data set contains patient records from a 1984 — 1989 trial conducted by the German Breast Cancer Study Group (GBSG) of 720 patients with node positive cancer…\n\n\n\n\nVariable\nExplanation\n\n\n\n\npid\nPatiend ID\n\n\nage\nAge in years\n\n\nmeno\nMenopausal staus - 0 = premenopaus, 1 = postmenopaus\n\n\nsize\ntumor size\n\n\ngrade\ntumor grade\n\n\nnodes\nnumber of positive lymph nodes\n\n\npgr\nprogesterone receptors (fmol/L)\n\n\ner\nestrogen receptors (fmol/L)\n\n\nhormon\nhormone therapy - 0 = no, 1 = yes\n\n\nrfstime\nrecurrence free survival time in days\n\n\nstatus\n0 = alive without recurrence, 1 = recurrence or death\n\n\n\nSelect variables included in Table 1.\n\nvariables &lt;- names(breast)\n\nbreast &lt;- breast %&gt;% select(all_of(variables)) %&gt;% \n  select(-pid, -rfstime, -status)\n\nbreast &lt;- breast %&gt;% mutate(meno = as_factor(meno), \n                            grade = as_factor(grade), \n                            hormon = as_factor(hormon))\n\nRename the variables as they should appear in the table. While tableone can add explanation for you, I genereally prefer to type them out myself to keep track of what I actually want to get (e.g. is it mean or median?)\n\nbreast &lt;- breast %&gt;% rename(`Age, years (mean (SD))` = age, \n                             Postmenopausal = meno, \n                             `Tumor size, mm (mean (SD)` = size, \n                             `Tumor grade` = grade, \n                             `Positive lymph nodes, (n)` = nodes, \n                             `Progesterone receptors, fmol/L (median [IQR])` = pgr, \n                             `Estrogen receptors, fmol/L (median [IQR])` = er, \n                             `Hormone treatment` = hormon)\n\nRelabel hormon variable to more text friendly labels\n\nbreast$`Hormone treatment` &lt;- breast$`Hormone treatment` %&gt;% \n  fct_recode(Treated = \"1\", Placebo = \"0\")\n\nSelect all variables and specify categorical variables for tableone\n\n# All variables excluding the group variable\nmyVars &lt;- breast %&gt;% select(-`Hormone treatment`) %&gt;% names() \n\n# All categorical variables\ncatVars &lt;-  breast %&gt;% select(where(is.factor)) %&gt;% \n  dplyr::select(-`Hormone treatment`) %&gt;% names()\n\nCreate Table 1 object\n\ntab1 &lt;- breast %&gt;% CreateTableOne(vars = myVars, \n                   data = . , \n                   factorVars = catVars, \n                   strata = \"Hormone treatment\", \n                   addOverall = T, \n                   test = T)\n\nPrint Table 1 object to control output, e.g. remove missing, print non-normal variables correctly etc.\n\ntab1_word &lt;- print(tab1, \n                   nonnormal = c(\"Progesterone receptors, fmol/L (median [IQR])\", \n                                 \"Estrogen receptors, fmol/L (median [IQR])\"),\n                   quote = F, \n                   noSpaces = T, \n                   # smd = T, \n                   # missing = T, \n                   test = F, \n                   contDigits = 1, \n                   printToggle = F,\n                   dropEqual = T, \n                   explain = F)\n\ncustom_tab() needs a dataframe as argument, so first we convert the tableone object\n\n# Convert to dataframe\ntab1_df &lt;- tab1_word %&gt;% as_tibble(rownames = \"Variable\")\n\nUse custom_tab() to export the MS Word table.\nImportant: run customtab_defaults() before using the custom_tab() function to get correct formatting\n\n# Rename first variable from n to No.\ntab1_df$Variable[1] &lt;- \"No.\"\n\n# Set Table header\nheader &lt;- str_squish(str_remove(\"Table 1. Baseline characteristics of 686 \n                                patients enrolled in the German Breast Cancer Study Group \n                                between 1984 and 1989\", \"\\n\"))\n\n# Set Table footer\nfooter &lt;- str_squish(str_remove(\"Numbers are No. (%) unless otherwise noted. \n                                SD = standard deviation, fmol/L = femtomole per liter, \n                                IQR = interquartile range\", \"\\n\"))\n\n# Set custom_tab() defaults\ncustomtab_defaults()\n\n# Create the flextable object\nflextable_1 &lt;- custom_tab(tab1_df, header, footer)\n\nflextable_1\n\nTable 1. Baseline characteristics of 686 patients enrolled in the German Breast Cancer Study Group between 1984 and 1989VariableOverallPlaceboTreatedNo.686440246Age, years (mean (SD))53.1 (10.1)51.1 (10.0)56.6 (9.4)Postmenopausal396 (57.7)209 (47.5)187 (76.0)Tumor size, mm (mean (SD)29.3 (14.3)29.6 (14.4)28.8 (14.1)Tumor grade   181 (11.8)48 (10.9)33 (13.4)   2444 (64.7)281 (63.9)163 (66.3)   3161 (23.5)111 (25.2)50 (20.3)Positive lymph nodes, (n)5.0 (5.5)4.9 (5.6)5.1 (5.3)Progesterone receptors, fmol/L (median [IQR])32.5 [7.0, 131.8]32.0 [7.0, 130.0]35.0 [7.2, 133.0]Estrogen receptors, fmol/L (median [IQR])36.0 [8.0, 114.0]32.0 [8.0, 92.2]46.0 [9.0, 182.5]Numbers are No. (%) unless otherwise noted. SD = standard deviation, fmol/L = femtomole per liter, IQR = interquartile range\n\n\nSave the resulting table as .docx\n\n# Save as word .docx\nsave_as_docx(flextable_1, path = \"flextab_1.docx\", \n             pr_section = \n               prop_section(page_size = page_size(orient = \"portrait\"), \n                                       type = \"continuous\"))\n\nThis will generate the following MS Word Table:\n\n\n\nCustom Table\n\n\nThe customtab_defaults() function simply sets some defaults for your tables, and can be changed to match the requirements of the journal you will be submitting to\n\ncustomtab_defaults &lt;- function(){\n  set_flextable_defaults(font.family = \"Calibri\", \n  font.size = 10, \n  border.color = \"black\")\n}\n\nThe custom_tab() function looks like this\n\n######### Create default BioAVR table from dataframe\n#\n# Dependencies : dplyr, flextable, officer\n#      \ncustom_tab &lt;- function(df, header, footer){\n  flextable(df) %&gt;% \n    add_header_lines(header) %&gt;% \n    add_footer_lines(footer) %&gt;% \n    bold(i = 1, part = \"header\") %&gt;% \n    hline_top(part = \"header\", \n              border = fp_border(color = \"red\", \n                                 width = 3, \n                                 style = \"solid\")) %&gt;% \n    hline(i = 1, \n          part = \"header\", \n          border = fp_border(color = \"black\", \n                             width = 0.25, \n                             style = \"solid\")) %&gt;% \n    hline_top(part = \"body\", \n              border = fp_border(color = \"black\", \n                                 width = 0.25, \n                                 style = \"solid\")) %&gt;% \n    hline_bottom(part = \"body\", \n                 border = fp_border(color = \"black\", \n                                    width = 0.25, \n                                    style = \"solid\")) %&gt;% \n    hline_bottom(part = \"footer\", \n                 border = fp_border(color = \"black\", \n                                    width = 0.25, \n                                    style = \"solid\")) %&gt;% \n    border_inner_h(part = \"body\", \n                   border = fp_border(color = \"black\", \n                                      width = 0.25, \n                                      style = \"dotted\")) %&gt;% \n    autofit(part = \"body\") %&gt;% \n    bg(part = \"body\", bg = \"#f5f5f5\") %&gt;% \n    align(part = \"all\", align = \"center\") %&gt;% \n    align(j = 1, part = \"all\", align = \"left\")\n}\n\nWhat it does is:\n1. Add header and footer\n2. Make header bold\n3. Adds red and black border for header\n4. Adds black borders for footer\n5. Adds dotted borders for body content\n6. Adds gray background to body\nThis is a standard design I go for in my tables, but it can of course be changed to your own or journal specific requirements. While flextable allows output to other formats as well, I generally only use this for MS Word output. The reason is that I’ve noticed problems with the borders when outputting to pdf or image formats."
  },
  {
    "objectID": "posts/implement-euroscore2-risk-score-in-r/index.html",
    "href": "posts/implement-euroscore2-risk-score-in-r/index.html",
    "title": "Implement EuroSCORE II risk score in R",
    "section": "",
    "text": "Today, I will show you a way to implement the EuroSCORE II risk score in R. EuroSCORE is a risk score that tries to predict mortality after cardiac surgery, and is used in clincal practice to assess individual risk before surgery. This helps inform the patient (and surgeon) about the expected risk, and can also contribute in the decision making whether the patient will likely benefit the most from surgery, or some other procedure such as transcatheter aortic valve implantation (TAVI), depending on the issue at hand and co-morbidities. If you just want to download function with same test data, you can do so here.\n\nEuroSCORE was created following a collaboration by researchers at Royal Papworth Hospital in Cambridge UK and the Centre Hospitalier Universitaire de Martinique.\n\nSince it’s conception, it has been updated several times. The latest version is called the EuroSCORE II, and it is a logistic regression model. As stated in the original publication of EuroSCORE II:\n\nCardiac surgical mortality has significantly reduced in the last 15 years despite older and sicker patients. EuroSCORE II is better calibrated than the original model yet preserves powerful discrimination. It is proposed for the future assessment of cardiac surgical risk.\n\nAs you might know, a logistic model estimates the \\(\\log\\text{odds}\\), and it’s linear predictor can be written as:\n\\[\\log\\text{odds} = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... \\beta_iX_i\\]\nConsequently, the \\(e^{\\beta_i}\\) can be interpreted as the odds ratio if all other covariates are held constant.\nLet’s also do a quick recap what odds actually are. Odds are usually expressed as \\(\\frac{events}{non-events}\\). Compare this to risk, which would be \\(\\frac{events}{events+non-events}\\). So, to convert odds to risk, we can use the formula \\(Risk=\\frac{Odds}{1+Odds}\\).\nLet’s now explore this using R.\n\n# Generate some example data\nset.seed(12345) # To make the results reproducible\nexample_data &lt;- data.frame(outcome = rbinom(100, 1, 0.5), # Generate a binary outcome\n           group = sample(c(\"A\", \"B\"), 100, replace = T)) # Generate a random group assignment\n\nNow, we can manually calculate the odds of the outcome per each group. Note that I will calculate risk using odds risk_1, and just using \\(Risk=\\frac{events}{non-events}\\). Of course, they are identical.\n\nlibrary(dplyr)\nlibrary(broom)\nexample_data %&gt;% group_by(group) %&gt;% \n  summarise(events = sum(outcome == 1), \n            non_events = sum(outcome == 0), \n            odds = events/non_events, \n            risk = events/(events+non_events), \n            risk_1 = odds/(1+odds))\n\n# A tibble: 2 × 6\n  group events non_events  odds  risk risk_1\n  &lt;chr&gt;  &lt;int&gt;      &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 A         26         21 1.24  0.553  0.553\n2 B         26         27 0.963 0.491  0.491\n\n\nNow let’s use a logistic regression model:\n\nreg_mod &lt;- glm(outcome ~ group, data = example_data, family = \"binomial\")\nsummary(reg_mod)\n\n\nCall:\nglm(formula = outcome ~ group, family = \"binomial\", data = example_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   0.2136     0.2934   0.728    0.467\ngroupB       -0.2513     0.4020  -0.625    0.532\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.47  on 99  degrees of freedom\nResidual deviance: 138.08  on 98  degrees of freedom\nAIC: 142.08\n\nNumber of Fisher Scoring iterations: 3\n\n\nHere we fitted a logistic regression model, where the coefficients can be interpreted as the log(odds). Now, let’s convert that to odds (or odds ratios):\n\ntidy(reg_mod) %&gt;% select(group = term, coef = estimate) %&gt;% \n  mutate(OR = exp(coef))\n\n# A tibble: 2 × 3\n  group         coef    OR\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 (Intercept)  0.214 1.24 \n2 groupB      -0.251 0.778\n\n\nThe (Intercept) here refers to Group A odds, but to obtain the odds for Group B we must add the coefficients together, otherwise it is the odds ratio of Group B over Group A.\n\nexp(reg_mod$coefficients[1]+reg_mod$coefficients[2])\n\n(Intercept) \n   0.962963 \n\n\nAll the numbers are the same as our manual calculation (although the output is rounded differently). Now let’s calculate the risk:\n\nexp(reg_mod$coefficients[1])/(1+exp(reg_mod$coefficients[1]))\n\n(Intercept) \n  0.5531915 \n\n\nThis function is actually built-in in the model object itself, and does the calculation much faster (which can be useful if you work with large data sets).\n\nreg_mod$family$linkinv(reg_mod$coefficients[1])\n\n(Intercept) \n  0.5531915 \n\n\nUsing the linear predictor, the odds of Group A is:\n\\[log_{odds}=\\beta_0 + \\beta_1Group_B*0\\] \\[log_{odds[Group A]} = 0.2136 + (-0.2513*0)\\]\nAnd consequently, the linear predictor for Group B is:\n\\[log_{odds}=\\beta_0 + \\beta_1Group_B*1\\]\n\\[log_{odds[Group B]} = 0.2136 + (-0.2513*1)\\]\nNow, let’s implement the results from the EuroSCORE II logistic regression model as a function to estimate the EuroSCORE II in our own data sets. Below are the coefficients as reported by the EuroSCORE II publication They modeled age non-linearly, as stated in the article:\n\nβi the coefficient of the variable Xi, for age, Xi = 1 if patient age ≤60; Xi increases by one point per year thereafter (age 60 or less Xi = 1; age 61 if Xi = 2; age 62 if Xi = 3 and so on).\n\n\n\n\nVariable.\nCoefficient\n\n\n\n\n\n\n\nNYHA\n\n\n\n\n\n\n II\n0.1070545\n\n\n\n\n\n III\n0.2958358\n\n\n\n\n\n IV\n0.5597929\n\n\n\n\n\nCCS4\n0.2226147\n\n\n\n\n\nIDDM\n0.3542749\n\n\n\n\n\nAge\n0.0285181\n\n\n\n\n\nFemale\n0.2196434\n\n\n\n\n\nECA\n0.5360268\n\n\n\n\n\nCPD\n0.1886564\n\n\n\n\n\nN/M mob\n0.2407181\n\n\n\n\n\nRedo\n01.118599\n\n\n\n\n\nRenal dysfunction\n\n\n\n\n\n\n On dialysis\n0.6421508\n\n\n\n\n\n CC ≤ 50\n0.8592256\n\n\n\n\n\n CC 50−85\n0.303553\n\n\n\n\n\nAE\n0.6194522\n\n\n\n\n\nCritical\n1.086517\n\n\n\n\n\nLV function\n\n\n\n\n\n\n Moderate\n0.3150652\n\n\n\n\n\n Poor\n0.8084096\n\n\n\n\n\n Very poor\n0.9346919\n\n\n\n\n\nRecent MI\n0.1528943\n\n\n\n\n\nPA systolic pressure\n\n\n\n\n\n\n 31–55 mmHg\n0.1788899\n\n\n\n\n\n ≥55\n0.3491475\n\n\n\n\n\nUrgency\n\n\n\n\n\n\n Urgent\n0.3174673\n\n\n\n\n\n Emergency\n0.7039121\n\n\n\n\n\n Salvage\n1.362947\n\n\n\n\n\nWeight of procedure\n\n\n\n\n\n\n 1 non-CABG\n0.0062118\n\n\n\n\n\n 2\n0.5521478\n\n\n\n\n\n 3+\n0.9724533\n\n\n\n\n\nThoracic aorta\n0.6527205\n\n\n\n\n\nConstant\n−5.324537\n\n\n\n\n\n\nUsing this information, we can calculate the EuroSCORE II in a function by using a series of mutate() and case_when() statements.\n\ncalc_euroscore &lt;- function(data.frame) {\n\n   euroscore &lt;- data.frame %&gt;% mutate(est_NYHA = case_when(EURO_NYHA == 2 ~ 0.1070545, \n                                              EURO_NYHA == 3 ~ 0.2958358,\n                                              EURO_NYHA == 4 ~ 0.5597929,\n                                              EURO_NYHA == 1 ~ 0,\n                                              EURO_NYHA == 9 ~ NA_real_),\n                                      est_ccs4 = case_when(EURO_CCS4 == 9 ~ NA_real_,\n                                                           TRUE ~ 0.2226147 * EURO_CCS4),\n                                      est_IDDM = case_when(is.na(DIABETESINSULIN) ~ 0,\n                                                           .default = 0.3542749 * DIABETESINSULIN),\n                                      est_age = case_when(EURO_AGE &lt;= 60 ~ 0.0285181,\n                                                          EURO_AGE &gt; 60 ~ (EURO_AGE - 59) * 0.0285181),\n                                      est_female = case_when(EURO_SEX == 2 ~ 0.2196434,\n                                                             EURO_SEX == 1 ~ 0),\n                                      est_eca = case_when(EURO_EXTRACARDIAC == 9 ~ NA_real_,\n                                                          TRUE ~ 0.5360268 * EURO_EXTRACARDIAC),\n                                      est_cpd = case_when(EURO_CHRONICPULM == 9 ~ NA_real_,\n                                                          TRUE ~ 0.1886564 * EURO_CHRONICPULM),\n                                      est_neuro = case_when(EURO_POORMOBILITY == 9 ~ NA_real_,\n                                                            TRUE ~ 0.2407181 * EURO_POORMOBILITY),\n                                      est_PREVCARDSURG = case_when(EURO_PREVCARDSURG == 9 ~ NA_real_,\n                                                               TRUE ~ 1.118599 * EURO_PREVCARDSURG),\n                                      est_cc = case_when(DIALYS == 1 ~ 0.6421508,\n                                                         EURO_CREATININE_CLEAREANCE &lt;= 50 ~ 0.8592256,\n                                                         EURO_CREATININE_CLEAREANCE &gt; 50 & EURO_CREATININE_CLEAREANCE &lt;= 85 ~ 0.303553,\n                                                         EURO_CREATININE_CLEAREANCE &gt; 85 ~ 0),\n                                      est_AE = case_when(EURO_ENDOCARDITIS == 9 ~ NA_real_,\n                                                         TRUE ~ 0.6194522 * EURO_ENDOCARDITIS),\n                                      est_critical = case_when(EURO_CRITICAL == 9 ~ NA_real_,\n                                                               TRUE ~ 1.086517 * EURO_CRITICAL),\n                                      est_LV = case_when(EURO_VK_EJFRACT2 == 0 ~ 0,\n                                                         EURO_VK_EJFRACT2 == 1 ~ 0.3150652,\n                                                         EURO_VK_EJFRACT2 == 2 ~ 0.8084096,\n                                                         EURO_VK_EJFRACT2 == 3 ~ 0.9346919),\n                                      est_hjinf = case_when(EURO_HJINF == 9 ~ NA_real_,\n                                                            TRUE ~ 0.1528943 * EURO_HJINF),\n                                      est_pulmonchoice = case_when(EURO_PULMONCHOICE == 0 ~ 0, \n                                                                   EURO_PULMONCHOICE == 1 ~ 0.1788899,\n                                                                   EURO_PULMONCHOICE == 2 ~ 0.3491475), \n                                      est_urgency = case_when(EURO_URGENCY == 1 ~ 0,\n                                                              EURO_URGENCY == 2 ~ 0.3174673, \n                                                              EURO_URGENCY == 3 ~ 0.7039121,\n                                                              EURO_URGENCY == 4 ~ 1.362947),\n                                      est_weightofprocedure = case_when(EURO_WEIGHTOFPROCEDURE == 0 ~ 0,\n                                                                        EURO_WEIGHTOFPROCEDURE == 1 ~ 0.0062118,\n                                                                        EURO_WEIGHTOFPROCEDURE == 2 ~ 0.5521478,\n                                                                        EURO_WEIGHTOFPROCEDURE == 3 ~ 0.9724533),\n                                      est_THORACALAORTA = case_when(EURO_THORACALAORTA == 9 ~ NA_real_,\n                                                               TRUE ~ 0.6527205 * EURO_THORACALAORTA),\n                                      est_sum = est_NYHA + est_ccs4 + est_IDDM + est_age + est_female + est_eca + est_cpd + est_neuro +\n                                                est_PREVCARDSURG + est_cc + est_AE + est_critical + est_LV + est_hjinf + est_pulmonchoice +\n                                                est_urgency + est_weightofprocedure + est_THORACALAORTA,\n                                      euro_new = exp(-5.324537 + est_sum)/(1 + exp(-5.324537 + est_sum)),\n                                      euro_new = round(euro_new * 100, 2)) %&gt;%\n                                      pull(euro_new)\n                                      \n\n                               return(euroscore)\n}\n\nLet’s try it out on a dummy data set:\n\ntest_data &lt;- data.frame(EURO_NYHA = c(2, 9),\n                             EURO_CCS4 = c(1, 1),\n                             DIABETESINSULIN = c(1, NA), \n                             EURO_AGE = c(70, 9), \n                             EURO_SEX = c(2, 2),  \n                             EURO_EXTRACARDIAC = c(1, 1),\n                             EURO_CHRONICPULM = c(1, 9),\n                             EURO_POORMOBILITY = c(1, 1), \n                             EURO_PREVCARDSURG = c(1, 1), \n                             DIALYS = c(1, 0), \n                             EURO_CREATININE_CLEAREANCE = c(40, 40), \n                             EURO_ENDOCARDITIS = c(1, 9), \n                             EURO_CRITICAL = c(1, 9), \n                             EURO_VK_EJFRACT2 = c(2, 2), \n                             EURO_HJINF = c(1, 1),\n                             EURO_PULMONCHOICE = c(1, 1),\n                             EURO_URGENCY = c(1, 1),\n                             EURO_WEIGHTOFPROCEDURE = c(1, 1),\n                             EURO_THORACALAORTA = c(1, 1))\n\ncalc_euroscore(test_data)\n\n[1] 89.32    NA\n\n\nThis is the same result as on the EuroSCORE II calculator:\n\n\n\nEuroSCORE II calculator\n\n\nA few closing remarks. You have to decide before hand how you want to handle missing data. In the current implementation, if anything is missing it will not calculate a EuroSCORE. In most cases, it might be reasonable to prepare the data set with replace_na() to chose some sort of default value. I would also like to thank our PhD student, Ruixin Lu for the help with developing this function when we needed it for one of our projects."
  },
  {
    "objectID": "posts/regression-standardization-with-mexhaz/index.html",
    "href": "posts/regression-standardization-with-mexhaz/index.html",
    "title": "Regression standardization with the mexhaz package",
    "section": "",
    "text": "In this post, I will demonstrate how to use the mexhaz package to estimate regression-standardized survival curves. In future posts, I will also cover how to achieve this using the rstpm2, stdReg, and marginaleffects packages for various outcome measures, including in a competing risk setting.\nFirst, let’s discuss what regression standardization is and why it is useful. As you may know, a common method to analyze and visualize survival data is by constructing a Kaplan-Meier curve. Since Kaplan-Meier estimates are crude, these curves are often combined with a hazard ratio from a Cox regression to determine if there is a statistically significant difference in survival between treatment groups.\nOne way to provide “adjusted” survival curves is by stratifying, i.e., creating several Kaplan-Meier curves based on a categorical variable, such as age over or under 50. While this approach might be appropriate in certain cases, it is not feasible to adjust for many covariates this way or for continuous covariates, as it would produce numerous curves with very few patients represented in each.\nAnother option is to calculate a survival curve from a hazard model, such as Cox regression. This involves fitting a model and using it to predict a survival curve at different follow-up times. For example, we could predict survival for a 50-year-old man with Treatment A and diabetes, and then draw the same curve for a 50-year-old man with Treatment B and diabetes. Although this estimates survival for a particular individual, it is incorrect to assume that using mean values (such as mean age) will yield a mean effect estimate for the population under study. To estimate the treatment effect in the population, we can instead use regression standardization.\nThis involves fitting a hazard model using the entire population. We then predict survival for each individual in the study under the alternative treatment options. First, we re-code everyone to have received Treatment A and predict individual estimates. Next, we assign everyone to have received Treatment B and predict individual estimates. Finally, we average the estimates at each time point within the group to obtain population mean estimates.\nNow, let’s implement this in R. We will start by simulating a dataset of 1000 patients with Treatment A or B, with or without diabetes, who either died or survived during follow-up.\n\nlibrary(dplyr)\nlibrary(forcats)\nset.seed(12345)\n\nn &lt;- 1000\n\ndata &lt;- data.frame(\n  age = c(sample(60:89, n/2, replace = TRUE), sample(45:74, n/2, replace = TRUE)),\n  treatment = rep(c(\"Treatment A\", \"Treatment B\"), each = n/2),\n  sex = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  diabetes = sample(0:1, n, replace = TRUE)\n) %&gt;%\n  # Shuffle the rows to mix Treatment A and Treatment B groups\n  slice_sample(n = n) %&gt;%\n  # Generate follow-up time, depending on treatment group and age. \n  mutate(follow_up_time = round(ifelse(treatment == \"Treatment A\",\n                                       rexp(n, rate = 1 / (1200 - (age - 45) * 4)),\n                                       rexp(n, rate = 1 / (1200 - (age - 45) * 8)))),\n         follow_up_time = ifelse(follow_up_time == 0, 1, follow_up_time), \n         follow_up_years = follow_up_time / 365.25,\n         # Event probability increases with age\n         event = rbinom(n, 1, prob = plogis((age - 45) / 10)))\n\nLet’s do a regular Kaplan-Meier analysis first.\n\nlibrary(survival)\nlibrary(ggsurvfit)\n\nLoading required package: ggplot2\n\nlibrary(ggpubr)\n\nmyCols &lt;- c(\"Difference\" = \"#7570b3\", \n            \"Treatment A\" = \"#1b9e77\", \n            \"Treatment B\" = \"#e7298a\")\n\nsurvfit2(Surv(follow_up_years, event) ~ treatment, data = data) %&gt;% \n  ggsurvfit(key_glyph = \"rect\", linewidth = 0.8) + \n  add_pvalue(location = \"annotation\") + \n  scale_ggsurvfit(x_scales = list(limits = c(0, 10), \n                                  breaks = seq(0, 10, by = 2))) + \n  theme_pubr() + \n  add_risktable() + \n  scale_color_manual(aesthetics = c(\"fill\", \"color\"), \n                     values = myCols, \n                     breaks = c(\"Treatment B\", \"Treatment A\"))\n\n\n\n\n\n\n\n\nHere it seems like patients who received Treatment B had better survival compared to patients who received Treatment A (p = 0.006).\n\ncoxph(Surv(follow_up_years, event) ~ treatment + age + diabetes, data = data)\n\nCall:\ncoxph(formula = Surv(follow_up_years, event) ~ treatment + age + \n    diabetes, data = data)\n\n                         coef exp(coef) se(coef)     z        p\ntreatmentTreatment B 0.196873  1.217589 0.091265 2.157    0.031\nage                  0.026941  1.027308 0.004122 6.536 6.32e-11\ndiabetes             0.108292  1.114373 0.069194 1.565    0.118\n\nLikelihood ratio test=53.02  on 3 df, p=1.819e-11\nn= 1000, number of events= 843 \n\n\nAfter adjusting for age using Cox regression, we now see that Treatment B is associated with an increased risk of death compared to Treatment A (HR: 1.22, p = 0.03). Not let’s analyse this using regression standardization. First, we fit a hazard model using the mexhaz() function. To read more about the various arguments, see ?mexhaz.\n\nlibrary(mexhaz)\n\nknots &lt;- quantile(data$follow_up_years, probs=c(1/3,2/3))\nmexmod &lt;- mexhaz(Surv(follow_up_years, event) ~ treatment + age + diabetes, \n                 data = data, \n                 base = \"exp.bs\", \n                 degree = 3, \n                 knots = knots, \n                 verbose = 0, \n                 print.level = 0)\n\n\nData\n Name N.Obs.Tot N.Obs N.Events N.Clust\n data      1000  1000      843       1\n\nDetails\n Iter Eval   Base Nb.Leg Nb.Aghq Optim Method Code    LogLik Total.Time\n    5    8 exp.bs     20      10   nlm    ---    1 -1950.525      0.037\n\n\nNext, we use the adjsurv function to perform the regression standardization.\n\nadjsurv_mexhaz &lt;- adjsurv(mexmod, time.pts = seq(0.25, 15, by = 0.25), \n                          data = transform(data, treatment = \"Treatment A\"), \n                          data.0 = transform(data, treatment = \"Treatment B\"))\n\nsurv_plot_data &lt;- tibble(time = adjsurv_mexhaz$results$time.pts,\n                  Est = adjsurv_mexhaz$results$adj.surv,\n                  L = adjsurv_mexhaz$results$adj.surv.sup,\n                  U = adjsurv_mexhaz$results$adj.surv.inf,\n                  treatment = \"Treatment A\") %&gt;%\n  bind_rows(tibble(time = adjsurv_mexhaz$results$time.pts,\n                   Est = adjsurv_mexhaz$results$adj.surv.0,\n                   L = adjsurv_mexhaz$results$adj.surv.0.sup,\n                   U = adjsurv_mexhaz$results$adj.surv.0.inf,\n                   treatment = \"Treatment B\")) %&gt;%\n  mutate(treatment = as.factor(treatment))\n\nsurv_diff_plot_data &lt;- tibble(time = adjsurv_mexhaz$results$time.pts,\n                   diff = adjsurv_mexhaz$results$diff.adj.surv,\n                   L = adjsurv_mexhaz$results$diff.adj.surv.inf,\n                   U = adjsurv_mexhaz$results$diff.adj.surv.sup)\n\nLet’s plot the results.\n\nsurv_fig &lt;- surv_plot_data %&gt;% ggplot(aes(x = time, y = Est)) +\n  geom_line(aes(col = treatment)) +\n  geom_ribbon(aes(ymin = L, ymax = U, fill = treatment), alpha = 0.6) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  theme_pubr() +\n  labs(x = \"Time (years)\") +\n  theme(axis.title.y = element_blank(),\n        legend.title = element_blank(),\n        legend.position.inside = c(0.85, 0.85),\n        text = element_text(size = 25)) +\n  scale_color_manual(aesthetics = c(\"fill\", \"color\"), values = myCols)\n\n\ndiff_fig &lt;- surv_diff_plot_data %&gt;% ggplot(aes(x = time, y = diff)) +\n  geom_line(col = myCols[\"Difference\"]) +\n  geom_ribbon(aes(ymin = L, ymax = U), alpha = 0.6, fill = myCols[\"Difference\"]) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  theme_pubr() +\n  labs(x = \"Time (years)\") +\n  theme(axis.title.y = element_blank(),\n        legend.title = element_blank(),\n        legend.position.inside = c(0.8, 0.8),\n        text = element_text(size = 25))\n\n\nsurv_comb &lt;- ggarrange(surv_fig, diff_fig, ncol = 1)\nsurv_comb\n\n\n\n\n\n\n\n\nThe lower figure plots the estimated difference in survival between Treatment A and Treatment B. The solid line represents the point estimate, and the shaded area indicates the 95% confidence interval. Since the difference does not cross the 0-line, we can reject the null hypothesis that there is no difference (i.e., the difference is zero).\nThis approach offers several advantages over simply reporting the hazard ratio (HR) from a Cox model. It provides estimates throughout the follow-up period, making it easier to interpret the treatment effect. Additionally, we can visualize how the difference evolves over time, often showing a trend toward a smaller difference at the end of the follow-up.\nThis example demonstrates a straightforward way to perform regression standardization. However, in a real analysis, one must consider various factors, such as model specification (e.g., variable selection, non-linear terms, interaction terms), handling of missing data, and the appropriateness of the method for the specific setting.\nIn the next post, we will explore how to achieve regression standardization using the rstpm2 package."
  },
  {
    "objectID": "posts/age-and-sex-adjusted-incidence-rates/index.html",
    "href": "posts/age-and-sex-adjusted-incidence-rates/index.html",
    "title": "Age-, and sex-adjusted incidence rates",
    "section": "",
    "text": "In this first post I’m going to present a way of obtaining age- and sex-adjusted incidence rates using poisson regression in R. This will be similar to what is done in Stata as described here.\nI’ve written a R function that’s available for download here. The script can be sourced ( source(\"age_sex_adjust.R\") ) and then the function age_sex_adjust() can be used as is. Note that the time variable will have to be in days, and the incidence will be presented as per 100 person-years. The code will also be described step by step below.\n\nThere are, as usual, several ways to calculate adjusted incidence rates in R. I’ve chosen to use the package marginaleffects by Vincent Arel-Bundock, Noah Greifer and Andrew Heiss because it has a lot of nice features and useful implications in causal inference. Specifically, we will use the function avg_predictions() from marginaleffects to generate the the adjusted incidence rates.\nBut first we start off with a little bit of background on what an incidence rate is. It is simply a measure of a number of occurrences (a count) in a population over the total population time. For example, in a population of 10 people, each followed 1 year, there was one case of death. In that population, the incidence rate of death would 1 per 10 person-years. In observational data, we often have larger cohorts with varying follow-up time and censoring. The calculation is of course the same, using the formula below:\n \\[\\text{Incidence rate} = \\frac{\\text{Number of occurrences}}{\\sum_{\\text{Persons}}{\\text{Time in study}}}\\] \n\n\nCalculating crude incidence rate\nTo illustrate, we will now use the colon dataset from the survival package.\n\nlibrary(survival)\nlibrary(dplyr)\nlibrary(broom)\n\nRunning ?survival::colon tells us the following:\n\nData from one of the first successful trials of adjuvant chemotherapy for colon cancer\n\n\n\n\n\n\n\n\nVariable\nExplanation\n\n\n\n\nid\nPatient id\n\n\nstudy\n1 for all patients\n\n\nrx\n1 for all patients\n\n\nsex\n1 = male\n\n\nage\nin years\n\n\nobstruct\ncolon obstruction by tumour\n\n\nperfor\nperformation of colon\n\n\nadhere\nadherence to nearby organs\n\n\nnodes\nnumber of positive lymph nodes\n\n\ntime\ndays until event or censoring\n\n\nstatus\ncensoring status\n\n\ndiffer\ntumour differentiation — 1 = well, 2 = moderate, 3 = poor\n\n\nextent\nextent of local spread — 1 = submucosa, 2 = muscle, 3 = serosa, 4 = continious\n\n\nsurg\ntime from surgery to registration — 0 = short, 1 = long\n\n\nnode4\nmore than 4 positive lymph nodes\n\n\netype\nevent type — 1 = recurrence, 2 = death\n\n\n\nOK, so now that we understand the data, let’s start calculating crude incidence rates for death among the different treatment groups:\n\n# Using the colon dataset from the survival package\n\n# Only keep records related to the death outcome\ncolon_death &lt;- survival::colon %&gt;% dplyr::filter(etype == 2) \n\n# Time is divided by 365.25/100 to get the time in days \n# first to years, then to 100 person-years\n\ncolon_death %&gt;% group_by(rx) %&gt;% \n                summarise(Events = sum(status), \n                          Time = sum(time/365.25/100), \n                          Rate = Events / Time, \n                          lower = poisson.test(Events, Time)$conf.int[1], \n                          upper = poisson.test(Events, Time)$conf.int[2])\n\n# A tibble: 3 × 6\n  rx      Events  Time  Rate lower upper\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Obs        168  13.8 12.2  10.4  14.2 \n2 Lev        161  13.7 11.7  10.0  13.7 \n3 Lev+5FU    123  15.0  8.22  6.83  9.80\n\n\nNow we compare to the calculated rates with rates obtained from the survRate() function from the biostat3 package:\n\nlibrary(biostat3)\nsurvRate(Surv(time/365.25/100, status) ~ rx, data = colon_death) %&gt;% \n  dplyr::select(rx, event, tstop, rate, lower, upper) %&gt;% \n  as_tibble() %&gt;% \n  dplyr::rename(Events = event, \n                Time = tstop, \n                Rate = rate)\n\n# A tibble: 3 × 6\n  rx      Events  Time  Rate lower upper\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Obs        168  13.8 12.2  10.4  14.2 \n2 Lev        161  13.7 11.7  10.0  13.7 \n3 Lev+5FU    123  15.0  8.22  6.83  9.80\n\n\nGood, the incidence rates are identical. The observational patients had an mortality incidence rate of 12.2 per 100 person-years, compared to the Lev+5-FU treated patients with an incidence rate of 8.22 per 100 person-years. Now, let’s try and repeat these results with poisson regression.\n\n\nObtaining estimated incidence rates using poisson regression\nHere we use the broom package tidy() function to obtain exponentiated estimates:\n\n# Fit the model to estimate IRR using offset\npoisson_fit &lt;- glm(status ~ rx + offset(log(time/365.25/100)), \n                   family = poisson, data = colon_death)\n\n# Exponentiate the estimate to obtain IRR\ntidy(poisson_fit, exponentiate = T, conf.int = T)\n\n# A tibble: 3 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   12.2      0.0772    32.4   3.13e-230   10.4      14.1  \n2 rxLev          0.965    0.110     -0.324 7.46e-  1    0.777     1.20 \n3 rxLev+5FU      0.675    0.119     -3.32  9.16e-  4    0.534     0.850\n\n\nThe Intercept estimate here is the estimated IR for the reference level, i.e. the Obs group.\nTo get estimated IR of Lev+5FU treated:\n\nlev_5fu &lt;- predict(poisson_fit, \n                   newdata = data.frame(rx = \"Lev+5FU\", time = 36525), \n                   type = \"link\", se.fit = T)\n\nas_tibble(lev_5fu) %&gt;% summarise(Treatment = \"Lev+5FU\", \n                                 IR = exp(fit), \n                                 lower = exp(fit - (1.96 * se.fit)), \n                                 upper = exp(fit + (1.96 * se.fit)))\n\n# A tibble: 1 × 4\n  Treatment    IR lower upper\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Lev+5FU    8.22  6.88  9.80\n\n\nHere, the confidence interval needs to be calculated on the \\(\\log\\) scale and then exponentiated back. This will cause the confidence interval to not be centered around the estimate.\nA poisson model can model \\(\\log\\text{incidence rates (ratios)}\\) when we use the time variable as an offset. Therefore, we can include covariates in the model to be accounted for, such as age and sex.\n\n\nAge- and sex-adjusted incidence rates using poisson regression\nFirst, we’ll do it using my age_sex_adjust() function\n\nsource(\"age_sex_adjust.R\")\n# Usage: age_sex_adjust(data, group, age, sex, event, time)\n\nage_sex_adjust(colon_death, rx, age, sex, status, time)\n\n  Treatment        IR  Lower_CI  Upper_CI\n1   Lev+5FU  8.245483  6.786801  9.704165\n2       Obs 12.211861 10.362612 14.061109\n3       Lev 11.819634  9.990693 13.648575\n\n\nHere we see that the adjusted rates are very similar to the crude rates calculated above. Since this data comes from a randomized trial, this is expected and can be taken as a sign that the randomization worked.\nNow, let’s do the some thing but without using the ready made function to see how it works under the hood.\n\nlibrary(marginaleffects)\n\n# Fit the model using offset to estimate IRR\npoisson_fit &lt;- glm(status ~ rx * I(age^2) + sex + offset(log(as.numeric(time))), \n                   data = colon_death, \n                   family = poisson)\n\n# Create a new dataset where time is converted to 36525 days (100 years)\nnewdat &lt;- colon_death %&gt;% mutate(time = 36525) \n\n# Use avg_predictions to estimate what the incidence rate would have been if \n# the entire population would have been treated according to each level of rx\nresult &lt;- avg_predictions(poisson_fit, \n                          variables = \"rx\", \n                          newdata = transform(colon_death, time = 36525), \n                          type = \"response\")\n\nresult %&gt;% dplyr::select(rx, estimate, conf.low, conf.high)\n\n\n Estimate CI low CI high\n     8.25   6.79     9.7\n    12.21  10.36    14.1\n    11.82   9.99    13.6\n\nColumns: rx, estimate, conf.low, conf.high \n\n\nThe numbers are identical to the ones obtained from the age_sex_adjust() function, which is logical since we did the same thing as the function does.\nA few finishing notes. Here I included age as a quadratic term, and as an interaction with exposure. These are modeling decisions one will have to take, however the model could have been only a main effects model such as:\n \\[\\log\\lambda = \\beta_0 + \\beta_1\\text{rxLev} + \\beta_2\\text{rxLev+5FU} + \\beta_3\\text{age} + \\beta_4\\text{sex}\\]  Regarding the interaction term, a good explanation was given in the Stata forum in this post.\nFor anyone who wants to read more, I recommend the course material from the PhD course Biostatistics III at Karolinska Institutet, available here."
  },
  {
    "objectID": "posts/regression-standardization-with-rstpm2/index.html",
    "href": "posts/regression-standardization-with-rstpm2/index.html",
    "title": "Regression standardization with the rstpm2 package",
    "section": "",
    "text": "This post will be another example of how to perform regression standardization in a survival analysis setting, this time using the rstpm2 package. In case you didn’t see the previous post that describes what regression standardization is, look here first.\nLet’s start be recreating the simulated data set from the last post.\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(rstpm2)\nlibrary(survival)\nlibrary(ggsurvfit)\nlibrary(ggpubr)\n\nset.seed(12345)\n\nn &lt;- 1000\n\ndata &lt;- data.frame(\n  age = c(sample(60:89, n/2, replace = TRUE), sample(45:74, n/2, replace = TRUE)),\n  treatment = as.factor(rep(c(\"Treatment A\", \"Treatment B\"), each = n/2)),\n  sex = sample(c(\"Male\", \"Female\"), n, replace = TRUE),\n  diabetes = sample(0:1, n, replace = TRUE)\n) %&gt;%\n  # Shuffle the rows to mix Treatment A and Treatment B groups\n  slice_sample(n = n) %&gt;%\n  # Generate follow-up time, depending on treatment group and age. \n  mutate(follow_up_time = round(ifelse(treatment == \"Treatment A\",\n                                       rexp(n, rate = 1 / (1200 - (age - 45) * 4)),\n                                       rexp(n, rate = 1 / (1200 - (age - 45) * 8)))),\n         follow_up_time = ifelse(follow_up_time == 0, 1, follow_up_time), \n         follow_up_years = follow_up_time / 365.25,\n         # Event probability increases with age\n         event = rbinom(n, 1, prob = plogis((age - 45) / 10)))\n\nAs last time, it seems like patients who received Treatment B had better survival compared to patients who received Treatment A (p = 0.006) in crude analysis.\n\nmyCols &lt;- c(\"Difference\" = \"#7570b3\", \n            \"Treatment A\" = \"#1b9e77\", \n            \"Treatment B\" = \"#e7298a\")\n\nsurvfit2(Surv(follow_up_years, event) ~ treatment, data = data) %&gt;% \n  ggsurvfit(key_glyph = \"rect\", linewidth = 0.8) + \n  scale_ggsurvfit(x_scales = list(limits = c(0, 10), \n                                  breaks = seq(0, 10, by = 2))) + \n  add_pvalue(location = \"annotation\") + \n  theme_pubr() + \n  add_risktable() + \n  scale_color_manual(aesthetics = c(\"fill\", \"color\"), \n                     values = myCols, \n                     breaks = c(\"Treatment B\", \"Treatment A\"))\n\n\n\n\n\n\n\n\nNow let’s see how the survival curves look after regression standardization.\n\n# Fit the stpm2 flexible parametric survival model\nstpm2_fit &lt;- stpm2(Surv(follow_up_years, event) ~ treatment + age + diabetes, \n                   data = data)\n\n# Create an empty list to then populate with the predictions given\n# the different levels of treatment\npredictions &lt;- list(1:length(levels(data$treatment)))\n\n# Loop over the different treatment levels, \n# performing prediction of type \"meansurv\" on each level\nfor(i in seq_along(levels(data$treatment))){\n  index &lt;-  levels(data$treatment)[i] \n  predictions[[i]]  &lt;- predict(stpm2_fit, \n                               type = \"meansurv\", \n                               newdata = transform(data, treatment = index), \n                               se.fit = T, \n                               grid = T, \n                               full = T) %&gt;% \n    bind_cols(treatment = index)\n}\n\n# Survival difference, newdata - exposed\n# In this case Treatment B - Treatment A\nsurv_diff_plot_data &lt;- predict(stpm2_fit, \n                    type = \"meansurvdiff\", grid = T, \n                    newdata = transform(data, treatment = \"Treatment B\"), \n                    exposed = function(df) transform(df, treatment = \"Treatment A\"), \n                    full = T, \n                    se.fit = T)\n\nLet’s plot the result.\n\nsurv_plot_data &lt;- bind_rows(predictions) %&gt;% \n  mutate(treatment = as.factor(treatment))\n\n\nsurv_fig &lt;- surv_plot_data %&gt;% \n  ggplot(aes(x = follow_up_years, y = Estimate)) +\n  geom_line(aes(col = treatment)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), \n              alpha = 0.6) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  scale_x_continuous(limits = c(0, 15)) + \n  theme_pubr() +\n  labs(x = \"Time (years)\") +\n  theme(axis.title.y = element_blank(),\n        legend.title = element_blank(),\n        legend.position.inside = c(0.85, 0.85),\n        text = element_text(size = 25)) +\n  scale_color_manual(aesthetics = c(\"fill\", \"color\"), values = myCols)\n\n\ndiff_fig &lt;- surv_diff_plot_data %&gt;% \n  ggplot(aes(x = follow_up_years, y = Estimate)) +\n  geom_line(col = myCols[\"Difference\"]) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), \n              alpha = 0.6, \n              fill = myCols[\"Difference\"]) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +\n  scale_x_continuous(limits = c(0, 15)) + \n  theme_pubr() +\n  labs(x = \"Time (years)\") +\n  theme(axis.title.y = element_blank(),\n        legend.title = element_blank(),\n        legend.position.inside = c(0.8, 0.8),\n        text = element_text(size = 25))\n\n\nsurv_comb &lt;- ggarrange(surv_fig, diff_fig, ncol = 1)\nsurv_comb\n\n\n\n\n\n\n\n\nSo after regression standardization adjusting for age, it seems like Treatment A is associated with significantly improved survival compared to Treatment B."
  },
  {
    "objectID": "guides/setup/setup_1.html",
    "href": "guides/setup/setup_1.html",
    "title": "Setup guide",
    "section": "",
    "text": "This guide is intended to help new members of our research group get their environments up and running, or old members who need to get new hardware up and running.\nIn it’s current form, it’s assumed you’re using a Mac computer. The guide will assume you are very new to computer concepts such as running terminal commands, and I will try to be as clear as possible.\nBefore starting, you must:\n\nRegister an account at GitHub\nKnow, or be able to access your GitHub and computer password\n\nThis guide will guide you through the following steps:\n\nDetermine if you use an Intel or Apple silicon Mac\nInstalling Homebrew\nInstalling gh-cli\nInstalling R, and chosing the correct version according to if you have Intel or Apple Silicon\nInstalling RStudio",
    "crumbs": [
      "Guides",
      "Setup Guide",
      "Setup guide"
    ]
  },
  {
    "objectID": "guides/setup/setup_2.html",
    "href": "guides/setup/setup_2.html",
    "title": "Intel or Apple Silicon",
    "section": "",
    "text": "“Intel or Apple Silicon”\nBefore we move on, you need to determine if your Mac uses Intel or Apple Silicon.\nTo do this, click on the  symbol in the upper left corner of the screen, and then click “About this Mac”.\n\n\n\nApple menu\n\n\n\n\n\nAbout this Mac\n\n\nYou have an Apple Silicon Mac if the first line says M1, M2 or M3.\nYou have an Intel Mac if it says something with Intel.",
    "crumbs": [
      "Guides",
      "Setup Guide",
      "Intel or Apple Silicon"
    ]
  },
  {
    "objectID": "guides/setup/setup_5.html",
    "href": "guides/setup/setup_5.html",
    "title": "Installing R",
    "section": "",
    "text": "Now, it’s time to install R.\nGo to https://cran.r-project.org and click on “Download R for macOS”.\nNow, select the correct downloader depending on if you had an Intel or Apple Silicon Mac as described here.\n\n\n\nSelect the correct downloader\n\n\nWhen the .pkg file has finished downloading, double-click it to start the installation. When finished, you can move the .pkg installer to the trash.",
    "crumbs": [
      "Guides",
      "Setup Guide",
      "Installing R"
    ]
  },
  {
    "objectID": "guides/setup/guides.html",
    "href": "guides/setup/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Here are section with guides. They are aimed more at members and students at our research group, and as such does not fit in in the blog section. However, they might still be of help to others who face similar problems.\nThe first section is intended to aid in setting up the necessary software to work with R efficiently.\nThe second section showcases a workflow I prefer when starting new projects.\nMore sections might follow"
  },
  {
    "objectID": "guides/collaborate/collaborate_2.html",
    "href": "guides/collaborate/collaborate_2.html",
    "title": "Syncing dependencies using renv",
    "section": "",
    "text": "Now we have cloned the project, and if the owner has used renv to track packages and versions, we can leverage this to get the same versions. Write renv::status() to see if the project uses renv (usually it will say your project is out of sync in the console as the project starts up), and then type renv::restore() to download and install all required packages in the same versions as logged in the lockfile.\n\n\n\nrenv::status()\n\n\nThen you do your work, remember to add any newly used (and installed) packages using renv::snapshot(), save your files, and close RStudio. Next, we will go through how to push up your changes and then avoid any conflicts in your code.",
    "crumbs": [
      "Guides",
      "Collaborate",
      "Syncing dependencies using renv"
    ]
  },
  {
    "objectID": "guides/guides.html",
    "href": "guides/guides.html",
    "title": "Guides",
    "section": "",
    "text": "Welcome to the guides page.",
    "crumbs": [
      "Guides",
      "Guides"
    ]
  },
  {
    "objectID": "guides/new_project/new_project_2.html",
    "href": "guides/new_project/new_project_2.html",
    "title": "Using the usethis package",
    "section": "",
    "text": "First, we use spotlight to open RStudio (unless it’s already open).\n\n\n\nOpen RStudio\n\n\nIn the upper right corner, it should say Project: (None). Now, we will use the usethis package to create a new project, track it with git, and upload it to GitHub.\n\n\n\n\n\n\nBeware\n\n\n\nFor the following to work, you must have the folder my_git and subfolder where subfolder should be your first name. See here at the bottom of the page if you have not yet created them.\n\n\nIn the console, write:\n\nusethis::create_project(\"my_git/subfolder/project_name\")\n\nThis will create and open a new R-project in ~/my_git/subfolder/project_name. The folder structure is up to you, personally, I like to keep all git-repos in sub-folders of my_git, and where subfolder is the name of the repo owner, so in this case it would be ~/my_git/Michael/project_name.\nIn the console of the newly opened R-Studio (which now should have project_name in the upper right corner), write:\n\nusethis::use_git()\n\n\n\n\nuse_git()\n\n\nNext, link it to Github (use option private = T if you want to create a private repo):\n\nusethis::use_github(private = T)\n\nNow you are ready to create your first file, press Shift+Command+pShift+Command+p and select Create a New R Script. I recommend keeping track of dependencies using renv, so install any packages needed by typing in the Console:\n\nrenv::install(\"dplyr\")\n\nWhen finishing for the day, register all new packages with renv::snapshot(), save and close R-Studio, and then in the terminal do git add ., git commit -m \"Beginning on my new project, and last git push.\nThat marks the end of this guide. In the following section, I will demonstrate a suggested workflow when being invited to collaborate on a project.",
    "crumbs": [
      "Guides",
      "New project",
      "Using the usethis package"
    ]
  }
]